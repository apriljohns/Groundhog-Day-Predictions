---
title: "Group Project"
author: "April Johns, Mollie Murphy, Josh Daniels"
date: "2025-11-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

pull in raw data 
```{r}
library(tidyverse)

groundhogs_raw <- read_csv('../data/groundhogs_2025.csv')
predictions_raw <- read_csv('../data/predictions_2025.csv')
```

exploring the data
```{r}
# # we have predictions from 1886 - 2023 
# predictions_raw %>% 
#   pull(year) %>% 
#   range()
# 
# # number of predictions (aka prognosticating groundhogs) over the years 
# predictions_raw %>% 
#   group_by(year) %>% 
#   count() %>% 
#   ggplot() +
#   geom_line(aes(x = year, y = n))
```

uptick in predictions around this time: 
year  n 

1979	5			
1980	7			
1981	7			
1982	9			
1983	10	

Coincidently, daily weather data seems to only date back to 1981, so maybe we subset our data to 1981-present? 


Here's a package/weather API that I found works well integrating with R: 

https://cran.r-project.org/web/packages/nasapower/vignettes/nasapower.html

exploring the package: 
```{r}
library("nasapower")

# # example from the documentation linked above 
# daily_single_ag <- get_power(
#   community = "ag",
#   lonlat = c(151.81, -27.48),
#   pars = c("RH2M", "T2M", "PRECTOTCORR"),
#   dates = c("1985-01-01", "1985-01-31"),
#   temporal_api = "daily")
# 
# daily_single_ag
# 
# # possible parameters for daily data 
# params <- query_parameters(community = 'ag', temporal_api = 'daily') 
# 
# # parameter names 
# params %>% 
#   names()
# 
# # open params list in another window to see overview 
# params %>% 
#   view()
# 
# # looking specifically at T2M --> this returns a list object with description 
# # I think this is what we want! 
# # daily average temp (C) at 2 meters above the surface 
# t2m <- query_parameters(pars = 'T2M', community = 'ag', temporal_api = 'daily')
# 
# t2m
```

using the nasapower package for our data 
```{r}
# map avg temp data to each latitude/longitude pair 
# avg_daily_temp <- groundhogs_raw %>% 
#   mutate(avg_temp = map2(.x = longitude, .y = latitude, 
#                          ~get_power(community = "ag",
#                                     lonlat = c(.x, .y),
#                                     pars = c("T2M"),
#                                     dates = c("1981-01-01", "2025-07-01"),
#                                     temporal_api = "daily")))
# 
# # unnest data 
# avg_daily_temp_unnested <- avg_daily_temp %>% 
#   select(id, slug, avg_temp) %>% 
#   unnest(avg_temp)
# 
# # save in data folder so we don't need to run it fresh every time (it takes a while)
# write_csv(avg_daily_temp_unnested, file = '../data/average_daily_temp_2025.csv')

# the code above only needs to be run once, but I'll leave it here for now so 
# that you can see where it came from 

# now we can just read it in like the others 
avg_daily_temp_df <- read_csv('../data/average_daily_temp_2025.csv')

```



Calculating the average temperature for each 6-week interval: 

For the preceding 6 weeks each year: 
- Groundhog Day is on February 2nd --> DOY = 33 (this col is in dataset, thankfully!)
- 6 weeks = 42 days 
- 42 - 33 = 9: we need the last 9 days of the preceding year 
- Therefore, we want to compute average temperature in the range: DOY-1: 357-366 (to include leap years), DOY: 1-33

For the 6 week interval in question:
- DOY between 34-75

For the following 6 weeks of spring: 
- DOY between 76-116 

```{r}
# data frame with average winter temp (6 weeks preceding groundhog day)
avg_winter_temp_df <- avg_daily_temp_df %>% 
  filter(DOY %in% c(1:33, 357:366)) %>% 
  mutate(groundhog_winter_year = if_else(DOY %in% c(357:366), YEAR + 1, YEAR)) %>% 
  group_by(id, groundhog_winter_year) %>% 
  summarise(avg_winter_temp = mean(T2M))

# data frame with average temp for prediction interval (6 weeks following groundhog day)
avg_prediction_temp_df <- avg_daily_temp_df %>% 
  filter(DOY %in% c(34:75)) %>% 
  group_by(id, YEAR) %>% 
  summarise(avg_prediction_temp = mean(T2M))

# data frame with average spring temp (6 weeks following prediction interval -- 6-12 weeks following groundhog day)
avg_spring_temp_df <- avg_daily_temp_df %>% 
  filter(DOY %in% c(75:116)) %>% 
  group_by(id, YEAR) %>% 
  summarise(avg_spring_temp = mean(T2M))
  
# join dataframes together, then compare which season prediction interval is closest to 
prediction_eval_df <- predictions_raw %>% 
  filter(year > 1980) %>% 
  left_join(avg_winter_temp_df, by = c('id', 'year' = 'groundhog_winter_year')) %>% 
  left_join(avg_prediction_temp_df, by = c('id', 'year' = 'YEAR')) %>% 
  left_join(avg_spring_temp_df, by = c('id', 'year' = 'YEAR')) 

```

#jd: calculate deltas between: (avg_prediction_temp - avg_winter_temp) and (avg_spring_temp - #avg_prediction_temp) as absolute values.

```{r}
#winter delta
prediction_eval_df <- prediction_eval_df %>% 
  mutate(delta_winter = abs(avg_winter_temp - avg_prediction_temp))

#spring delta
prediction_eval_df <- prediction_eval_df %>% 
  mutate(delta_spring = abs(avg_spring_temp - avg_prediction_temp))

```

#jd: If delta_winter < delta_spring, then there was 6 more weeks of winter according to our model. Make another
new column relecting TRUE/FALSE for the above relationship.
Then, add another column to reflect correct or incorrect status of prognostication based on an 'and' relationship between 
the 'shadow' column and the 'six_more_weeks' column.
```{r}
#make the column for six more weeks based on our model
prediction_eval_df <- prediction_eval_df %>% 
  mutate(six_more_weeks = delta_winter < delta_spring)

#make the column for correct vs incorrect prognostication
prediction_eval_df <- prediction_eval_df %>%
  mutate(is_correct = if_else(shadow & six_more_weeks, TRUE, FALSE))
```

#calculate the rolling average for accuracy
```{r}
# #group by groundhog
# rolling_avg <- prediction_eval_df %>% 
#   group_by(id) %>% 
#   arrange(year) %>% 
#   mutate(rolling_mean = (is_correct +
#               lag(is_correct, 1) +
#               lag(is_correct, 2) +
#               lag(is_correct, 3) +
#               lag(is_correct, 4)) / 5) %>% 
#   ungroup()




```

Koppen climate zones are generated by the Rscript kgc_standalone_script
That library does not get along with this script, so a .csv file was generated
separately with that script that drops into the data folder and then we pick
it up here below without having to load the kgc library here.
```{r}

#read in koppen stand alone CSV generated by standalone scrip
koppen_legit <- read_csv("../data/koppen_zones.csv")

#join koppen_legit with the groundhogs

a <- left_join(x = groundhogs_raw, y = koppen_legit, by = "id")

#join with the prediction eval df

b <- left_join(x = prediction_eval_df, a, by = "id")

#get what is needed for the plot from this unholy mess of a df

c <- b %>% 
  select(climate_type, name, is_correct, six_more_weeks)


```
Another idea instead of the mvg_avg is to just plot average accuracy by 
Koppen climate zone or perhaps other climate zone classification systems?? 
Dataframe "b" above has all of the needed columns to do this.
Cool idea? Add columns for avg delta winter and delta spring for each climate
description.

Image could be bar graph with prognosticators grouped within climate type.
each of the respective climate types can also display a bar each for the deltas
above. Or.. Instead of a bar for each of the deltas, display a bar for 
"local model consistency" defined as proportion of years where "6 more weeks" 
by our model is true.

Hold the phone - found an actual R package that can fetch Koppen climate zones
"KGC"


```{r}
#summarize data by Koppen classification and name

#accuracy index is accuracy (range 0 - 1)/ consistency (range 0 - 0.5)
#What this does is gives an indicator of how accurate the prognosticator is
#adjusted for consistency of climate as defined by our "6 more weeks model".
#Do bar charts with accuracy, consistency, and accuracy index faceted by
#Koppen Zone.

#Summarize mean accuracy, season consistency and adjusted accuracy.
#Given that the most inconsistent "six more weeks" is defined as 50/50,
#we are taking the difference from 0.5 to score consistency, so the lower the
#number, the more INCONSISTENT. However, in wanting to calculate an adjusted
#accuracy, we are taking 1 minus this value to use as a multiplier so the
#higher the number, the more inconsistent, which is more useful as a multiplier
#for creating an adjusted accuracy calculation for our prognosticators.

# Awesome idea from Molly - arrange in quartiles
#Create the summary dataframe of our dreams
accuracy_plot_data <- c %>%
  group_by(name) %>% 
  summarise(accuracy = sum(is_correct, na.rm = TRUE)/n(),
            consistency = abs(0.5 - sum(six_more_weeks)/n()),
            adj_accuracy = accuracy * (1 - consistency)) %>% 
  arrange(consistency) %>% #ascending order for consistency
  ungroup()

ranks <- c(1:88) #vector for ranks and bind below

accuracy_plot_data_ranked <- cbind(accuracy_plot_data, ranks)

#determine cuts for quartiles
consist_quartiles <- accuracy_plot_data_ranked %>% pull(consistency) %>% 
  quantile()

#apply quartile cuts to data
foo <- accuracy_plot_data_ranked %>% 
  mutate(quartile = cut(consistency,
          breaks=c(0.02941176, 0.32121212, 0.40909091, 0.45454545, 0.5), 
          include.lowest=TRUE, 
          labels=c("0", "25", "50", "75"), 
          ordered_result = TRUE))
         

#Divide into consistency quartiles - these will just show
#accuracy and adjusted accuracy
#Do scatterplot of accuracy vs consistency

#tidy the data. Koppen zones are no longer needed.


pl_accuracy_plot_data <- foo %>%
  pivot_longer(cols = c(accuracy, consistency, adj_accuracy),
               names_to = "prognosticator_and_climate_parameter",
               values_to = "coefficients")

# Separate the datasets into each parameter

df_accuracy <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "accuracy")

df_adj_accuracy <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "adj_accuracy")

# Create the plot for the quartiles
ggplot() +
  geom_col(data = df_accuracy,
           aes(x = reorder(name, -coefficients),
               y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = 0), width = 0.35) +
  geom_col(data = df_adj_accuracy,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = 0.0), width = 0.35) +
  facet_wrap(~ quartile, scales = "free_x") +
  labs(
    title = "Consistency quartiles",
    x = "Name",
    y = "Coefficient Value",
    fill = "Metric"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  )

 
#Create the plot and for consistency vs accuracy
foo %>% ggplot(aes(x = consistency, y = accuracy, colour = name)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, colour = "black") +
  theme(legend.position = "none")


                    
                    
                   
```

```{r}
#break tidy df into sub-dfs for separate figure generation
tnds <- pl_accuracy_plot_data %>% 
  filter(climate_type == "Temperate No Dry Season Hot Summer")

#separate into components
# Separate the datasets into each parameter
tnds_consistency <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "consistency")

tnds_accuracy <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "accuracy")

tnds_adj_accuracy <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "adj_accuracy")

# Create the plot
tdns_plot <- ggplot() +
  geom_col(data = tnds_consistency,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = -0.2), width = 0.35) +
  geom_col(data = tnds_accuracy,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = 0.2), width = 0.35) +
  geom_col(data = tnds_adj_accuracy,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = 0.2), width = 0.35) +
  labs(
    title = "TNDS ONLY",
    x = "Name",
    y = "Coefficient Value",
    fill = "Metric"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )
  


#### OTHER BELOW

not_tnds <- pl_accuracy_plot_data %>% 
  filter(!climate_type == "Temperate No Dry Season Hot Summer")

# Separate the datasets into each parameter
not_tnds_consistency <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "consistency")

not_tnds_accuracy <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "accuracy")

not_tnds_adj_accuracy <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "adj_accuracy")

# Create the plot
not_tdns_plot <- ggplot() +
  geom_col(data = not_tnds_consistency,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = -0.2), width = 0.35) +
  geom_col(data = not_tnds_accuracy,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = 0.2), width = 0.35) +
  geom_col(data = not_tnds_adj_accuracy,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = 0.2), width = 0.35) +
  facet_wrap(~ climate_type, scales = "free_x") +
  labs(
    title = "Not_TNDS ONLY",
    x = "Name",
    y = "Coefficient Value",
    fill = "Metric"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  )


#try to do a fancy layout with the egg
library(egg)

ggarrange(tdns_plot, not_tdns_plot, heights = 1:2)

```








