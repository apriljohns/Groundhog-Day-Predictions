---
title: "Group Project"
author: "April Johns, Mollie Murphy, Josh Daniels"
date: "2025-11-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

pull in raw data 
```{r}
library(tidyverse)

groundhogs_raw <- read_csv('../data/groundhogs_2025.csv')
predictions_raw <- read_csv('../data/predictions_2025.csv')
```

exploring the data
```{r}
# # we have predictions from 1886 - 2023 
# predictions_raw %>% 
#   pull(year) %>% 
#   range()
# 
# # number of predictions (aka prognosticating groundhogs) over the years 
# predictions_raw %>% 
#   group_by(year) %>% 
#   count() %>% 
#   ggplot() +
#   geom_line(aes(x = year, y = n))
```

uptick in predictions around this time: 
year  n 

1979	5			
1980	7			
1981	7			
1982	9			
1983	10	

Coincidently, daily weather data seems to only date back to 1981, so maybe we subset our data to 1981-present? 


Here's a package/weather API that I found works well integrating with R: 

https://cran.r-project.org/web/packages/nasapower/vignettes/nasapower.html

exploring the package: 
```{r}
library("nasapower")

# # example from the documentation linked above 
# daily_single_ag <- get_power(
#   community = "ag",
#   lonlat = c(151.81, -27.48),
#   pars = c("RH2M", "T2M", "PRECTOTCORR"),
#   dates = c("1985-01-01", "1985-01-31"),
#   temporal_api = "daily")
# 
# daily_single_ag
# 
# # possible parameters for daily data 
# params <- query_parameters(community = 'ag', temporal_api = 'daily') 
# 
# # parameter names 
# params %>% 
#   names()
# 
# # open params list in another window to see overview 
# params %>% 
#   view()
# 
# # looking specifically at T2M --> this returns a list object with description 
# # I think this is what we want! 
# # daily average temp (C) at 2 meters above the surface 
# t2m <- query_parameters(pars = 'T2M', community = 'ag', temporal_api = 'daily')
# 
# t2m
```

using the nasapower package for our data 
```{r}
# map avg temp data to each latitude/longitude pair 
# avg_daily_temp <- groundhogs_raw %>% 
#   mutate(avg_temp = map2(.x = longitude, .y = latitude, 
#                          ~get_power(community = "ag",
#                                     lonlat = c(.x, .y),
#                                     pars = c("T2M"),
#                                     dates = c("1981-01-01", "2025-07-01"),
#                                     temporal_api = "daily")))
# 
# # unnest data 
# avg_daily_temp_unnested <- avg_daily_temp %>% 
#   select(id, slug, avg_temp) %>% 
#   unnest(avg_temp)
# 
# # save in data folder so we don't need to run it fresh every time (it takes a while)
# write_csv(avg_daily_temp_unnested, file = '../data/average_daily_temp_2025.csv')

# the code above only needs to be run once, but I'll leave it here for now so 
# that you can see where it came from 

# now we can just read it in like the others 
avg_daily_temp_df <- read_csv('../data/average_daily_temp_2025.csv')

```



Calculating the average temperature for each 6-week interval: 

For the preceding 6 weeks each year: 
- Groundhog Day is on February 2nd --> DOY = 33 (this col is in dataset, thankfully!)
- 6 weeks = 42 days 
- 42 - 33 = 9: we need the last 9 days of the preceding year 
- Therefore, we want to compute average temperature in the range: DOY-1: 357-366 (to include leap years), DOY: 1-33

For the 6 week interval in question:
- DOY between 34-75

For the following 6 weeks of spring: 
- DOY between 76-116 

```{r}
# data frame with average winter temp (6 weeks preceding groundhog day)
avg_winter_temp_df <- avg_daily_temp_df %>% 
  filter(DOY %in% c(1:33, 357:366)) %>% 
  mutate(groundhog_winter_year = if_else(DOY %in% c(357:366), YEAR + 1, YEAR)) %>% 
  group_by(id, groundhog_winter_year) %>% 
  summarise(avg_winter_temp = mean(T2M))

# data frame with average temp for prediction interval (6 weeks following groundhog day)
avg_prediction_temp_df <- avg_daily_temp_df %>% 
  filter(DOY %in% c(34:75)) %>% 
  group_by(id, YEAR) %>% 
  summarise(avg_prediction_temp = mean(T2M))

# data frame with average spring temp (6 weeks following prediction interval -- 6-12 weeks following groundhog day)
avg_spring_temp_df <- avg_daily_temp_df %>% 
  filter(DOY %in% c(75:116)) %>% 
  group_by(id, YEAR) %>% 
  summarise(avg_spring_temp = mean(T2M))
  
# join dataframes together, then compare which season prediction interval is closest to 
prediction_eval_df <- predictions_raw %>% 
  filter(year > 1980) %>% 
  left_join(avg_winter_temp_df, by = c('id', 'year' = 'groundhog_winter_year')) %>% 
  left_join(avg_prediction_temp_df, by = c('id', 'year' = 'YEAR')) %>% 
  left_join(avg_spring_temp_df, by = c('id', 'year' = 'YEAR')) 

```

#jd: calculate deltas between: (avg_prediction_temp - avg_winter_temp) and (avg_spring_temp - #avg_prediction_temp) as absolute values.

```{r}
#winter delta
prediction_eval_df <- prediction_eval_df %>% 
  mutate(delta_winter = abs(avg_winter_temp - avg_prediction_temp))

#spring delta
prediction_eval_df <- prediction_eval_df %>% 
  mutate(delta_spring = abs(avg_spring_temp - avg_prediction_temp))

```

#jd: If delta_winter < delta_spring, then there was 6 more weeks of winter according to our model. Make another
new column relecting TRUE/FALSE for the above relationship.
Then, add another column to reflect correct or incorrect status of prognostication based on an 'and' relationship between 
the 'shadow' column and the 'six_more_weeks' column.
```{r}
#make the column for six more weeks based on our model
prediction_eval_df <- prediction_eval_df %>% 
  mutate(six_more_weeks = delta_winter < delta_spring)

#make the column for correct vs incorrect prognostication
prediction_eval_df <- prediction_eval_df %>%
  mutate(is_correct = if_else(shadow & six_more_weeks, TRUE, FALSE))
```

#calculate the rolling average for accuracy
```{r}
# #group by groundhog
# rolling_avg <- prediction_eval_df %>% 
#   group_by(id) %>% 
#   arrange(year) %>% 
#   mutate(rolling_mean = (is_correct +
#               lag(is_correct, 1) +
#               lag(is_correct, 2) +
#               lag(is_correct, 3) +
#               lag(is_correct, 4)) / 5) %>% 
#   ungroup()




```
#join with groundhog names for IDs, because that will be nice for the graphic
```{r}
# rolling_avg_hognames <- rolling_avg %>% 
#   left_join(groundhogs_raw %>%
#               select(id, slug, region), by = "id")
# 
# #reorder columns
# rolling_avg_hognames <- rolling_avg_hognames %>% 
#   relocate(slug, .after = id) %>% 
#   relocate(region, .after = slug)


#make the widget
Back to the drawing board after some less than ideal plots in the pre-2025
version of the script. Do accuracy divided by region of country.  Have to find
1) A traditional way of dividing states into regions, put that into a df
Try using the Koppen-Geiger system to classify based on lat and long.
2) Mutate a new column with the region. Change name of "region" in original 
data to "state". 
which is really state in the original data.
3) Do a plotly that selects regions worth of plot lines and grays out the
unselected regions. This may have to use gghighlight w/ ggplotly

Koppen Zones are looked up with the kgc library - see L266
This legit method will replace the AI generated Koppen DF


Plan is to:
1. Join Koppen climate zones with groundhogs_2025
2. Use Plotly to show moving averages grouped by Koppen climate zone
3. Ideally use gghighlight to gray-out the unselected groups

# ```{r}
# library(kgc)
# 
# #requirement to pass a 3 column dataframe with just id, lat, long
# #see https://cran.r-project.org/web/packages/kgc/kgc.pdf
# data_kgc <- groundhogs_raw %>% 
#   select(id, longitude, latitude)
# 
# #pass data_kgc into the function
# koppen_from_kgc <- data.frame(data_kgc,
#                              rndCoord.lon  = RoundCoordinates(data_kgc$longitude),
#                              rndCoord.lat = RoundCoordinates(data_kgc$latitude))
# koppen_from_kgc <- data.frame(koppen_from_kgc,ClimateZ=LookupCZ(koppen_from_kgc))
#                              
# #Need a table with the Koppen definitions
# unique(koppen_from_kgc$ClimateZ)
# us_can_koppen_lookup <- tibble(
#   koppen_code = c("BSk", "BWk", "Cfa", "Cfb", "Csb",
#                   "Dfa", "Dfb", "Dfc"),
#   climate_type = c("Dry Steppe Cold", "Dry Desert Cold", 
#                    "Temperate No Dry Season Hot Summer",
#                    "Temperate No Dry Season Warm Summer",
#                    "Temperate Dry Summer Cold Summer", 
#                    "Continental No Dry Season Hot Summer", 
#                    "Continental No Dry Season Warm Summer",
#                    "Continental No Dry Season Cold Summer"))


#read in koppen stand alone CSV generated by standalone scrip
koppen_legit <- read_csv("../data/koppen_zones.csv")

#join koppen_legit with the groundhogs

a <- left_join(x = groundhogs_raw, y = koppen_legit, by = "id")

#join with the prediction eval df

b <- left_join(x = prediction_eval_df, a, by = "id")

#get what is needed for the plot from this unholy mess of a df

c <- b %>% 
  select(climate_type, name, is_correct, six_more_weeks)


```
Another idea instead of the mvg_avg is to just plot average accuracy by 
Koppen climate zone or perhaps other climate zone classification systems?? 
Dataframe "b" above has all of the needed columns to do this.
Cool idea? Add columns for avg delta winter and delta spring for each climate
description.

Image could be bar graph with prognosticators grouped within climate type.
each of the respective climate types can also display a bar each for the deltas
above. Or.. Instead of a bar for each of the deltas, display a bar for 
"local model consistency" defined as proportion of years where "6 more weeks" 
by our model is true.

Hold the phone - found an actual R package that can fetch Koppen climate zones
"KGC"


```{r}
#summarize data by Koppen classification and name

#accuracy index is accuracy (range 0 - 1)/ consistency (range 0 - 0.5)
#What this does is gives an indicator of how accurate the prognosticator is
#adjusted for consistency of climate as defined by our "6 more weeks model".
#Do bar charts with accuracy, consistency, and accuracy index faceted by
#Koppen Zone.

#Summarize mean accuracy, season consistency and adjusted accuracy.
#Given that the most inconsistent "six more weeks" is defined as 50/50,
#we are taking the difference from 0.5 to score consistency, so the lower the
#number, the more INCONSISTENT. However, in wanting to calculate an adjusted
#accuracy, we are taking 1 minus this value to use as a multiplier so the
#higher the number, the more inconsistent, which is more useful as a multiplier
#for creating an adjusted accuracy calculation for our prognosticators.


#Create the summary dataframe of our dreams
accuracy_plot_data <- c %>% 
  group_by(climate_type, name) %>% 
  summarise(accuracy = sum(is_correct, na.rm = TRUE)/n(),
            consistency = abs(0.5 - sum(six_more_weeks)/n()),
            adj_accuracy = accuracy * (1 - consistency)) %>% 
  arrange(climate_type, desc(adj_accuracy)) %>% 
  ungroup()





#generate a bar plot with each prognosticator having a pair of bars:
# 1 - consistency
# 2 - stacked with accuracy and adjusted accuracy
#plot will be faceted by Koppen Zone

#Tidy that S@!t up
pl_accuracy_plot_data <- accuracy_plot_data %>%
  pivot_longer(cols = c(accuracy, consistency, adj_accuracy),
               names_to = "prognosticator_and_climate_parameter",
               values_to = "coefficients")

#try to plot it - yikes

# Separate the datasets into each parameter
df_consistency <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "consistency")

df_accuracy <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "accuracy")

df_adj_accuracy <- pl_accuracy_plot_data %>%
  filter(prognosticator_and_climate_parameter == "adj_accuracy")

# Create the plot
ggplot() +
  geom_col(data = df_consistency,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = -0.2), width = 0.35) +
  geom_col(data = df_accuracy,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = 0.2), width = 0.35) +
  geom_col(data = df_adj_accuracy,
           aes(x = name, y = coefficients, fill = prognosticator_and_climate_parameter),
           position = position_nudge(x = 0.2), width = 0.35) +
  facet_wrap(~ climate_type, scales = "free_x") +
  labs(
    title = "Prognosticator XYZ",
    x = "Name",
    y = "Coefficient Value",
    fill = "Metric"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  )


                    
                    
                   
```


